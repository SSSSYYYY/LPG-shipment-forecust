{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the libraries\n",
    "\n",
    "import urllib2, urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib2\n",
    "import json\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "risk_all = []\n",
    "for i in range(1, 532): \n",
    "    path = '/Users/yangshenyang/Google\\ Drive/course/web\\ data/project/output/'+str(i)+'_risk.txt'\n",
    "    risk = open(path)\n",
    "    a= risk.read()\n",
    "    risk_all.append(a)\n",
    "# print risk_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dis_all = []\n",
    "for i in range(1, 532): # range(1,532)\n",
    "    path = '/Users/yangshenyang/Google\\ Drive/course/web\\ data/project/output/'+str(i)+'_discussion.txt'\n",
    "    dis = open(path)\n",
    "    a= dis.read()\n",
    "    dis_all.append(a)\n",
    "# print dis_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "market_risk_all = []\n",
    "for i in range(1, 532): # range(1,532)\n",
    "    path = '/Users/yangshenyang/Google\\ Drive/course/web\\ data/project/output/'+str(i)+'_market_risk.txt'\n",
    "    mr = open(path)\n",
    "    mrr= mr.read()\n",
    "    market_risk_all.append(mrr)\n",
    "# print market_risk_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== risk 0 started ===\n",
      "data total length 47942 => 11 chunks\n",
      "keyPhrases failed URL open, skip, error message:  HTTP Error 401: Access Denied\n",
      "sentiment failed URL open, skip, error message:  HTTP Error 401: Access Denied\n",
      "keyPhrases failed URL open, skip, error message:  HTTP Error 401: Access Denied\n",
      "sentiment failed URL open, skip, error message:  HTTP Error 401: Access Denied\n",
      "keyPhrases failed URL open, skip, error message:  HTTP Error 401: Access Denied\n",
      "sentiment failed URL open, skip, error message:  HTTP Error 401: Access Denied\n",
      "keyPhrases failed URL open, skip, error message:  HTTP Error 401: Access Denied\n",
      "sentiment failed URL open, skip, error message:  HTTP Error 401: Access Denied\n",
      "keyPhrases failed URL open, skip, error message:  HTTP Error 401: Access Denied\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4a0b73ec1bcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sentiment_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0msenti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongxia/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongxia/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongxia/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 447\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongxia/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongxia/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m             return self.do_open(httplib.HTTPSConnection, req,\n\u001b[0;32m-> 1241\u001b[0;31m                 context=self._context)\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongxia/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_selector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# XXX what error?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongxia/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_content_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongxia/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m   1080\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhdr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongxia/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongxia/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmessage_body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0mmessage_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0;31m#message_body was not a string (i.e. it is a file) and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongxia/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongxia/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0;34m\"Connect to a host on a given (SSL) port.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m             \u001b[0mHTTPConnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongxia/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;34m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         self.sock = self._create_connection((self.host,self.port),\n\u001b[0;32m--> 821\u001b[0;31m                                            self.timeout, self.source_address)\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongxia/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongxia/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mmeth\u001b[0;34m(name, self, *args)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socketmethods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# convert list of risk factor into list of score\n",
    "scores_three_section = []\n",
    "sec_names = ['risk', ' discussion', 'market risk']\n",
    "for sec_id, section_data in enumerate([risk_all, dis_all, market_risk_all]):\n",
    "    scores = []\n",
    "    for i, data in enumerate(section_data):\n",
    "        print \"\\n=== %s %i started ===\" % (sec_names[sec_id], i)\n",
    "        if data == 'N/A':\n",
    "            print 'data N/A, skipped'\n",
    "            scores.append('N/A')\n",
    "            continue\n",
    "\n",
    "        sentiment1 = []\n",
    "        keyphrases1 = []\n",
    "        total_keys1 = []\n",
    "        avg_score1 = []\n",
    "        final1 = []\n",
    "\n",
    "        step = 4000 # has to be less than 5000\n",
    "        num_step = int(len(data)/step)\n",
    "        if num_step == 0:\n",
    "            num_step = 1 # if length < 4000, just enforce 1 chunk\n",
    "\n",
    "        print \"data total length %s => %i chunks\" % (len(data), num_step)\n",
    "        for j in range(num_step):\n",
    "            parts = data[j*step:(j+1)*step]\n",
    "            parts = parts.encode('ascii',errors='ignore')\n",
    "            parts = parts.replace(\"'\", \"\").replace('\"', '') # remove single and double quotes\n",
    "            #Here we have broken down our text in to step character chunks\n",
    "            \n",
    "            # Azure portal URL.\n",
    "            base_url = 'https://westus.api.cognitive.microsoft.com/'\n",
    "            # The unique account key goes here.\n",
    "            account_key = '9ad81715e6b2480f91711867a2a9e8c1'    # 10,000 calls a month free\n",
    "            headers = {'Content-Type':'application/json', 'Ocp-Apim-Subscription-Key':account_key}\n",
    "            input_texts = '{\"documents\":[{\"id\":\"1\",\"text\":\"'+parts+'\"}]}' \n",
    "            #Here, instead of manually inputting any codes, we have put in the 2000 character chunk\n",
    "\n",
    "            # Detect key phrases.\n",
    "            batch_keyphrase_url = base_url + 'text/analytics/v2.0/keyPhrases'\n",
    "            req = urllib2.Request(batch_keyphrase_url, input_texts, headers) \n",
    "            try:\n",
    "                response = urllib2.urlopen(req)\n",
    "                result = response.read()\n",
    "                keyph = json.loads(result)\n",
    "                for phrases in keyph['documents']:\n",
    "                    keyphrases1.append(phrases['keyPhrases'])\n",
    "                # print \"phrases['keyPhrases']\", phrases['keyPhrases']\n",
    "            except Exception as e: \n",
    "                print \"keyPhrases failed URL open, skip, error message: \", e\n",
    "                \n",
    "            # Detect sentiment.\n",
    "            batch_sentiment_url = base_url + 'text/analytics/v2.0/sentiment'\n",
    "            req = urllib2.Request(batch_sentiment_url, input_texts, headers) \n",
    "            try:\n",
    "                response = urllib2.urlopen(req)\n",
    "                result = response.read()\n",
    "                senti = json.loads(result)\n",
    "                for sentiment_analysis in senti['documents']:\n",
    "                    sentiment1.append(sentiment_analysis['score']) # The sentiment scores are appended to the sentiment list\n",
    "                print \"data %i, chunk %i/%i, sentiment %.3f\" % (i, j, num_step, sentiment_analysis['score'])\n",
    "            except Exception as e: \n",
    "                print \"sentiment failed URL open, skip, error message: \", e\n",
    "                \n",
    "        # The keyphrases list will be a list within a list, containing a set of keywords obtained after every single run of 2000\n",
    "        # characters. So now, we will iterate over these inner lists to get each of the keywords into a final list\n",
    "\n",
    "        for items in keyphrases1:\n",
    "            for x in items:\n",
    "                final1.append(x)\n",
    "\n",
    "        unique_keys = list(set(final1)) #Using set functions, we clean the list to remove duplicates\n",
    "\n",
    "        total_keys1.append(unique_keys) #In this list we will be appending the list of unique keywords for each of the 10K submissions\n",
    "\n",
    "        avg_senti = np.mean(sentiment1) #Here we are calculate the average sentiment score for each of the chunks\n",
    "\n",
    "        avg_score1.append(avg_senti) # Storing the average sentiment score in the average score list\n",
    "\n",
    "        print \"avg_senti calculated from %i/%i chunks: %f\"% (len(sentiment1), num_step, avg_senti)\n",
    "        scores.append(avg_score1)\n",
    "\n",
    "    print '----final average scores for all section %s----' % sec_names[sec_id]\n",
    "    print scores\n",
    "    scores_three_section.append(scores)\n",
    "\n",
    "print '----final average scores for all 3 section data----'\n",
    "print scores_three_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob(market_risk_all[1])\n",
    "nlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dis_all = []\n",
    "for i in range(1, 532): # range(1,532)\n",
    "    path = '/Users/yangshenyang/Google\\ Drive/course/web\\ data/project/output/'+str(i)+'_discussion.txt'\n",
    "    dis = open(path)\n",
    "    a= dis.read()\n",
    "    dis_all.append(a)\n",
    "print dis_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('test.txt',header=None) # Here we are reading the 10K submission links\n",
    "\n",
    "links = list(df[0])\n",
    "links = ['https://www.sec.gov/Archives/edgar/data/1050797/000105079717000013/colm-20161231x10k.htm#s37E44109873A5A479DBABAB66B750F24']\n",
    "print links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the list of item 1A: risk factors\n",
    "\n",
    "risk1 = []\n",
    "\n",
    "for i in range(len(links)):\n",
    "    html = urllib2.urlopen(links[i]).read()\n",
    "    print html\n",
    "#     index = re.search('item 1a', html,flags=re.IGNORECASE) # searches the submission file for the exact mention of the term '1A'\n",
    "#     print '1st index', index.start()\n",
    "#     html = html[index.start()+3:] #cuts the html after the first 1A occurence, which is after the table of contents\n",
    "#     index = re.search('item 1a', html,flags=re.IGNORECASE) # finds 1A in the html after the table of contents\n",
    "#     print '2nd index', index.start()\n",
    "#     html = html[index.start():] # cuts off the previous html part\n",
    "#     index = re.search('item 1b', html,flags=re.IGNORECASE) # finds the exact occurence of 1B\n",
    "#     print \"3rd index:\", index\n",
    "#     \n",
    "#     risk1.append(html[:index.start()]) # stores the start of 1A to start of 1B in a string\n",
    "\n",
    "risk2 = []\n",
    "\n",
    "for i in range(len(risk1)):\n",
    "    soup = BeautifulSoup(risk1[i])\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text() # getting all the text from the soup\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    #whenever there is an empty string, the following if condition would render false\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    risk2.append(text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "risk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the list of item 7: Management Discussion\n",
    "\n",
    "md1 = []\n",
    "\n",
    "for i in range(len(links)):\n",
    "    html = urllib2.urlopen(links[i]).read()\n",
    "    index = re.search('Item 7.', html) # searches the submission file for the exact mention of the term 'Item 7.'\n",
    "    html = html[index.start()+3:] #cuts the html after the first Item 7. occurence, which is after the table of contents\n",
    "    index = re.search('ITEM 7.', html) # finds ITEM 7. in the html after the table of contents\n",
    "    html = html[index.start():] # cuts off the previous html part\n",
    "    index = re.search('ITEM 7A.', html) # finds the exact occurence of ITEM 7A.\n",
    "    md1.append(html[:index.start()]) # stores the start of 1A to start of 1B in a string\n",
    "\n",
    "md2 = []\n",
    "\n",
    "for i in range(len(md1)):\n",
    "    soup = BeautifulSoup(md1[i])\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text() # getting all the text from the soup\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    #whenever there is an empty string, the following if condition would render false\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    md2.append(text.encode('utf-8'))\n",
    "\n",
    "md2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the list of item 7A: Market Risk\n",
    "\n",
    "market1 = []\n",
    "\n",
    "for i in range(len(links)):\n",
    "    html = urllib2.urlopen(links[i]).read()\n",
    "    index = re.search('Item 7A.', html) # searches the submission file for the exact mention of the term 'Item 7.'\n",
    "    html = html[index.start()+3:] #cuts the html after the first Item 7. occurence, which is after the table of contents\n",
    "    index = re.search('ITEM 7A.', html) # finds ITEM 7. in the html after the table of contents\n",
    "    html = html[index.start():] # cuts off the previous html part\n",
    "    index = re.search('ITEM 8.', html) # finds the exact occurence of ITEM 7A.\n",
    "    market1.append(html[:index.start()]) # stores the start of 1A to start of 1B in a string\n",
    "\n",
    "market2 = []\n",
    "\n",
    "for i in range(len(market1)):\n",
    "    soup = BeautifulSoup(market1[i])\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text() # getting all the text from the soup\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    #whenever there is an empty string, the following if condition would render false\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    market2.append(text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert list of Management Discussion into list of score\n",
    "\n",
    "for i in range(len(md2)):\n",
    "    \n",
    "    sentiment2 = []\n",
    "    keyphrases2 = []\n",
    "    total_keys2 = []\n",
    "    avg_score2 = []\n",
    "    \n",
    "    final2 = []\n",
    "    text = md2[i]\n",
    "    for x in range(0, len(text), 2000):\n",
    "        if len(text)-x > 2000:\n",
    "            parts = text[x:x+2000]\n",
    "        else:\n",
    "            parts = text[x:]\n",
    "\n",
    "        #Here we have broken down our text in to 2000 character chunks\n",
    "\n",
    "        # Azure portal URL.\n",
    "        base_url = 'https://westus.api.cognitive.microsoft.com/'\n",
    "        # The unique account key goes here.\n",
    "        account_key = '9ad81715e6b2480f91711867a2a9e8c1'\n",
    "\n",
    "        headers = {'Content-Type':'application/json', 'Ocp-Apim-Subscription-Key':account_key}\n",
    "\n",
    "        input_texts = '{\"documents\":[{\"id\":\"1\",\"text\":\"'+parts+'\"}]}' \n",
    "        #Here, instead of manually inputting any codes, we have put in the 2000 character chunk\n",
    "\n",
    "        # Detect key phrases.\n",
    "        batch_keyphrase_url = base_url + 'text/analytics/v2.0/keyPhrases'\n",
    "        req = urllib2.Request(batch_keyphrase_url, input_texts, headers) \n",
    "        response = urllib2.urlopen(req)\n",
    "        result = response.read()\n",
    "        keyph = json.loads(result)\n",
    "\n",
    "        for phrases in keyph['documents']:\n",
    "            keyphrases2.append(phrases['keyPhrases'])\n",
    "\n",
    "        # Detect sentiment.\n",
    "        batch_sentiment_url = base_url + 'text/analytics/v2.0/sentiment'\n",
    "        req = urllib2.Request(batch_sentiment_url, input_texts, headers) \n",
    "        response = urllib2.urlopen(req)\n",
    "        result = response.read()\n",
    "        senti = json.loads(result)\n",
    "\n",
    "        for sentiment_analysis in senti['documents']:\n",
    "            sentiment2.append(sentiment_analysis['score']) # The sentiment scores are appended to the sentiment list\n",
    "\n",
    "    # The keyphrases list will be a list within a list, containing a set of keywords obtained after every single run of 2000\n",
    "    # characters. So now, we will iterate over these inner lists to get each of the keywords into a final list\n",
    "\n",
    "    for items in keyphrases2:\n",
    "        for x in items:\n",
    "            final2.append(x)\n",
    "\n",
    "    unique_keys = list(set(final2)) #Using set functions, we clean the list to remove duplicates\n",
    "\n",
    "    total_keys2.append(unique_keys) #In this list we will be appending the list of unique keywords for each of the 10K submissions\n",
    "\n",
    "    avg_senti = np.mean(sentiment2) #Here we are calculate the average sentiment score for each of the chunks\n",
    "\n",
    "    avg_score2.append(avg_senti) # Storing the average sentiment score in the average score list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert list of Management Discussion into list of score\n",
    "\n",
    "for i in range(len(market2)):\n",
    "    \n",
    "    sentiment3 = []\n",
    "    keyphrases3 = []\n",
    "    total_keys3 = []\n",
    "    avg_score3 = []\n",
    "    \n",
    "    final3 = []\n",
    "    text = market2[i]\n",
    "    for x in range(0, len(text), 2000):\n",
    "        if len(text)-x > 2000:\n",
    "            parts = text[x:x+2000]\n",
    "        else:\n",
    "            parts = text[x:]\n",
    "\n",
    "        #Here we have broken down our text in to 2000 character chunks\n",
    "\n",
    "        # Azure portal URL.\n",
    "        base_url = 'https://westus.api.cognitive.microsoft.com/'\n",
    "        # The unique account key goes here.\n",
    "        account_key = '01f5b19c9bd9425ab5c17dfaf2a497ed'\n",
    "\n",
    "        headers = {'Content-Type':'application/json', 'Ocp-Apim-Subscription-Key':account_key}\n",
    "\n",
    "        input_texts = '{\"documents\":[{\"id\":\"1\",\"text\":\"'+parts+'\"}]}' \n",
    "        #Here, instead of manually inputting any codes, we have put in the 2000 character chunk\n",
    "\n",
    "#         # Detect key phrases.\n",
    "        batch_keyphrase_url = base_url + 'text/analytics/v2.0/keyPhrases'\n",
    "#         req = urllib2.Request(batch_keyphrase_url, input_texts, headers) \n",
    "#         response = urllib2.urlopen(req)\n",
    "#         result = response.read()\n",
    "#         keyph = json.loads(result)\n",
    "\n",
    "#         for phrases in keyph['documents']:\n",
    "#             keyphrases3.append(phrases['keyPhrases'])\n",
    "\n",
    "#         # Detect sentiment.\n",
    "        batch_sentiment_url = base_url + 'text/analytics/v2.0/sentiment'\n",
    "#         req = urllib2.Request(batch_sentiment_url, input_texts, headers) \n",
    "#         response = urllib2.urlopen(req)\n",
    "#         result = response.read()\n",
    "#         senti = json.loads(result)\n",
    "\n",
    "#         for sentiment_analysis in senti['documents']:\n",
    "#             sentiment3.append(sentiment_analysis['score']) # The sentiment scores are appended to the sentiment list\n",
    "\n",
    "        import requests\n",
    "        tufani={\n",
    "             \"documents\": [\n",
    "                 {\n",
    "                     \"id\": \"1\",\n",
    "                     \"text\": parts\n",
    "                 },\n",
    "             ]\n",
    "         }\n",
    "        r=requests.post(batch_sentiment_url,data=tufani,headers =headers)\n",
    "        print(r.status_code, r.reason)\n",
    "\n",
    "\n",
    "        \n",
    "    # The keyphrases list will be a list within a list, containing a set of keywords obtained after every single run of 2000\n",
    "    # characters. So now, we will iterate over these inner lists to get each of the keywords into a final list\n",
    "\n",
    "    for items in keyphrases2:\n",
    "        for x in items:\n",
    "            final2.append(x)\n",
    "\n",
    "    unique_keys = list(set(final3)) #Using set functions, we clean the list to remove duplicates\n",
    "\n",
    "    total_keys3.append(unique_keys) #In this list we will be appending the list of unique keywords for each of the 10K submissions\n",
    "\n",
    "    avg_senti = np.mean(sentiment3) #Here we are calculate the average sentiment score for each of the chunks\n",
    "\n",
    "    avg_score3.append(avg_senti) # Storing the average sentiment score in the average score list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_score3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Azure portal URL.\n",
    "base_url = 'https://westus.api.cognitive.microsoft.com/'\n",
    "# Your account key goes here.\n",
    "account_key = '01f5b19c9bd9425ab5c17dfaf2a497ed'\n",
    "\n",
    "headers = {'Content-Type':'application/json', 'Ocp-Apim-Subscription-Key':account_key}\n",
    "            \n",
    "input_texts = '{\"documents\":[{\"id\":\"1\",\"text\":\"This was one of the best restaurants I have been to.  the food was amazing!\"},\\\n",
    "                {\"id\":\"2\",\"text\":\"This restaurant was ok.  Nothing to write home about.\"},\\\n",
    "                {\"id\":\"three\",\"text\":\"This was the worst restaurant I have been to.  Truly horrible.\"}]}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
